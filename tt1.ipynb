{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b630c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_experiment.py\n",
    "\"\"\"\n",
    "Top-level script to:\n",
    "  1) Generate synthetic family data for multi-cancer risk modeling\n",
    "  2) Instantiate environment, models, and RL agent\n",
    "  3) Train the RL agent for hereditary cancer screening\n",
    "  4) Evaluate performance with comprehensive metrics\n",
    "  5) Save trained models (policy & risk) and synthetic data\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from core.environment import HereditaryCancerEnv\n",
    "from core.agent import CancerScreeningAgent\n",
    "from core.models import make_policy_network, make_risk_model\n",
    "from core.simulation_data_generator import generate_data\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\"Hereditary Cancer Screening RL\")\n",
    "    parser.add_argument('--episodes', type=int, default=500,\n",
    "                        help='Number of training episodes')\n",
    "    parser.add_argument('--eval_episodes', type=int, default=100,\n",
    "                        help='Number of evaluation episodes')\n",
    "    parser.add_argument('--n_individuals', type=int, default=3,\n",
    "                        help='Family size')\n",
    "    parser.add_argument('--max_stage', type=int, default=3,\n",
    "                        help='Maximum tumor stage (M)')\n",
    "    parser.add_argument('--horizon', type=int, default=10,\n",
    "                        help='Time steps per episode')\n",
    "    parser.add_argument('--policy_type', type=str, choices=['mlp','rnn','transformer'],\n",
    "                        default='mlp', help='Policy network architecture')\n",
    "    parser.add_argument('--risk_type', type=str, choices=['feed','rnn'],\n",
    "                        default='feed', help='Risk model architecture')\n",
    "    parser.add_argument('--gamma', type=float, default=0.99,\n",
    "                        help='Discount factor')\n",
    "    parser.add_argument('--device', type=str, default=None,\n",
    "                        help='cpu or cuda (auto if not set)')\n",
    "    parser.add_argument('--save_dir', type=str, default='checkpoints',\n",
    "                        help='Directory to save trained models and data')\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                        help='Random seed for data generation and simulation')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def train_agent(args):\n",
    "    device = args.device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    env = HereditaryCancerEnv(\n",
    "        n_individuals=args.n_individuals,\n",
    "        M=args.max_stage,\n",
    "        max_time=args.horizon,\n",
    "        device=device\n",
    "    )\n",
    "    feat_dim = 32\n",
    "    risk_model = make_risk_model(\n",
    "        args.risk_type,\n",
    "        obs_dim=env.obs_dim,\n",
    "        feat_dim=feat_dim\n",
    "    ).to(device)\n",
    "    policy_net = make_policy_network(\n",
    "        args.policy_type,\n",
    "        input_dim=feat_dim,\n",
    "        n_individuals=args.n_individuals\n",
    "    ).to(device)\n",
    "    agent = CancerScreeningAgent(\n",
    "        policy=policy_net,\n",
    "        risk_model=risk_model,\n",
    "        gamma=args.gamma,\n",
    "        device=device\n",
    "    )\n",
    "    for ep in range(1, args.episodes+1):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.select_action(obs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            agent.store_reward(reward)\n",
    "        agent.update_policy()\n",
    "        if ep % 50 == 0:\n",
    "            print(f\"[Train] Episode {ep}/{args.episodes} complete.\")\n",
    "    return agent\n",
    "\n",
    "\n",
    "def evaluate_agent(agent, args):\n",
    "    env = HereditaryCancerEnv(\n",
    "        n_individuals=args.n_individuals,\n",
    "        M=args.max_stage,\n",
    "        max_time=args.horizon,\n",
    "        device=agent.device\n",
    "    )\n",
    "    total_returns = []\n",
    "    detection_times = []\n",
    "    detection_stages = []\n",
    "    cumulative_costs = []\n",
    "    for _ in range(args.eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        t = 0\n",
    "        episode_return = 0.0\n",
    "        detected = [False]*args.n_individuals\n",
    "        detect_time = [args.horizon+1]*args.n_individuals\n",
    "        detect_stage = [None]*args.n_individuals\n",
    "        cost = 0.0\n",
    "        while not done:\n",
    "            action = agent.select_action(obs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            episode_return += reward\n",
    "            cost += len(action) * env.screening_cost\n",
    "            for idx in action:\n",
    "                if not detected[idx] and env.last_tests[idx] == 1:\n",
    "                    detected[idx] = True\n",
    "                    detect_time[idx] = t\n",
    "                    detect_stage[idx] = env.stages[idx]\n",
    "            t += 1\n",
    "        total_returns.append(episode_return)\n",
    "        cumulative_costs.append(cost)\n",
    "        for i in range(args.n_individuals):\n",
    "            if detected[i]:\n",
    "                detection_times.append(detect_time[i])\n",
    "                detection_stages.append(detect_stage[i])\n",
    "    avg_return = np.mean(total_returns)\n",
    "    std_return = np.std(total_returns)\n",
    "    avg_cost = np.mean(cumulative_costs)\n",
    "    det_rate = len(detection_times)/(args.eval_episodes*args.n_individuals)\n",
    "    avg_det_time = np.mean(detection_times) if detection_times else np.nan\n",
    "    avg_det_stage = np.mean(detection_stages) if detection_stages else np.nan\n",
    "    print(f\"[Eval] Return: {avg_return:.2f} ± {std_return:.2f}\")\n",
    "    print(f\"[Eval] Avg cost: {avg_cost:.2f}\")\n",
    "    print(f\"[Eval] Detection rate: {det_rate*100:.1f}%\")\n",
    "    print(f\"[Eval] Avg detection time: {avg_det_time:.2f}\")\n",
    "    print(f\"[Eval] Avg detection stage: {avg_det_stage:.2f}\")\n",
    "    return {\n",
    "        'avg_return': avg_return,\n",
    "        'std_return': std_return,\n",
    "        'avg_cost': avg_cost,\n",
    "        'detection_rate': det_rate,\n",
    "        'avg_detection_time': avg_det_time,\n",
    "        'avg_detection_stage': avg_det_stage\n",
    "    }\n",
    "\n",
    "\n",
    "def save_models(agent, args):\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    # Export policy state dict to JSON\n",
    "    policy_sd = agent.policy.state_dict()\n",
    "    policy_json = {k: policy_sd[k].cpu().numpy().tolist() for k in policy_sd}\n",
    "    with open(os.path.join(args.save_dir, 'policy.json'), 'w') as f:\n",
    "        json.dump(policy_json, f)\n",
    "    # Export risk model state dict to JSON\n",
    "    risk_sd = agent.risk_model.state_dict()\n",
    "    risk_json = {k: risk_sd[k].cpu().numpy().tolist() for k in risk_sd}\n",
    "    with open(os.path.join(args.save_dir, 'risk_model.json'), 'w') as f:\n",
    "        json.dump(risk_json, f)\n",
    "    print(f\"Models saved in JSON to {args.save_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80b11d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data...\n",
      "Synthetic data saved.\n",
      "[Train] Episode 50/500 complete.\n",
      "[Train] Episode 100/500 complete.\n",
      "[Train] Episode 150/500 complete.\n",
      "[Train] Episode 200/500 complete.\n",
      "[Train] Episode 250/500 complete.\n",
      "[Train] Episode 300/500 complete.\n",
      "[Train] Episode 350/500 complete.\n",
      "[Train] Episode 400/500 complete.\n",
      "[Train] Episode 450/500 complete.\n",
      "[Train] Episode 500/500 complete.\n",
      "[Eval] Return: 17.28 ± 29.00\n",
      "[Eval] Avg cost: 15.82\n",
      "[Eval] Detection rate: 42.7%\n",
      "[Eval] Avg detection time: 4.98\n",
      "[Eval] Avg detection stage: 1.22\n",
      "Models saved in JSON to checkpoints\n",
      "Metrics saved.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    print(\"Generating synthetic data...\")\n",
    "    df = generate_data(\n",
    "        family_id=0,\n",
    "        T=args.horizon,\n",
    "        M=args.max_stage,\n",
    "        seed=args.seed\n",
    "    )\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    df.to_csv(os.path.join(args.save_dir, 'synthetic_data.csv'), index=False)\n",
    "    print(\"Synthetic data saved.\")\n",
    "    agent = train_agent(args)\n",
    "    metrics = evaluate_agent(agent, args)\n",
    "    save_models(agent, args)\n",
    "    pd.DataFrame([metrics]).to_csv(os.path.join(args.save_dir, 'performance_metrics.csv'), index=False)\n",
    "    print(\"Metrics saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ba29c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
